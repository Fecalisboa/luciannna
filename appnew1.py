# Pacote para manipula√ß√£o dos dados em formato JSON
import json

# Framework para cria√ß√£o de aplica√ß√µes web
import streamlit as st  

# Para cria√ß√£o e execu√ß√£o de agentes conversacionais
from langchain.agents import ConversationalChatAgent, AgentExecutor  

# Callback para intera√ß√£o com a interface do Streamlit
from langchain_community.callbacks import StreamlitCallbackHandler  

# Mem√≥ria para armazenar o hist√≥rico de conversa
from langchain.memory import ConversationBufferMemory  

# Hist√≥rico de mensagens para o Streamlit
from langchain_community.chat_message_histories import StreamlitChatMessageHistory 

# Integra√ß√£o com o modelo de linguagem da Cohere
from langchain_cohere import ChatCohere
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Ferramenta de busca DuckDuckGo para o agente 
from langchain_community.tools import DuckDuckGoSearchRun  

# Remove warnings
import warnings
warnings.filterwarnings('ignore')

# Pacotes para processamento de documentos PDF e cria√ß√£o de banco de dados vetorial
import os
from pathlib import Path
import re
from unidecode import unidecode
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_community.llms import HuggingFaceEndpoint

# Configura√ß√£o do t√≠tulo da p√°gina
st.set_page_config(page_title="lucIAna")

# Cria√ß√£o de colunas para layout da p√°gina
# Define a propor√ß√£o das colunas
col1, col4 = st.columns([4, 1])  

# Configura√ß√£o da primeira coluna para exibir o t√≠tulo do projeto
with col1:
    st.title("lucIAna")

# Defini√ß√£o da chave de API da Cohere
cohere_api_key = "OGY2ZCgZ4351TM0pXzRNeJLpw6o9GhyfWA3r05eW"

# Obtenha o token da vari√°vel de ambiente
hf_api_key = "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"

# Adi√ß√£o de bot√µes para diferentes funcionalidades
st.sidebar.header("Escolha uma op√ß√£o:")
option = st.sidebar.radio("Op√ß√µes", ["IA - CHAT", "IA - Docs"])

# Fun√ß√£o para carregar dados do arquivo JSON
def load_data(file_path):
    try:
        with open(file_path, 'r') as file:
            return json.load(file)
    except FileNotFoundError:
        return []

# Fun√ß√£o para salvar dados no arquivo JSON
def save_data(file_path, data):
    with open(file_path, 'w') as file:
        json.dump(data, file, indent=4)

# Caminho do arquivo JSON
json_file_path = 'chat_history.json'

# Carregar hist√≥rico de mensagens do arquivo JSON
chat_history = load_data(json_file_path)

# Inicializa√ß√£o do hist√≥rico de mensagens no Streamlit
if "msgs" not in st.session_state:
    st.session_state.msgs = StreamlitChatMessageHistory()

msgs = st.session_state.msgs

# Configura√ß√£o da mem√≥ria do chat
memory = ConversationBufferMemory(chat_memory=msgs, 
                                  return_messages=True, 
                                  memory_key="chat_history", 
                                  output_key="output")

# Verifica√ß√£o para limpar o hist√≥rico de mensagens ou iniciar a conversa
if len(msgs.messages) == 0 ou st.sidebar.button("Reset", key="reset_button"):
    msgs.clear()
    msgs.add_ai_message("Sou sua Assistente Jur√≠dica, em que posso ajudar?")
    st.session_state.steps = {}

# Defini√ß√£o de avatares para os participantes da conversa
avatars = {"human": "user", "ai": "üë©‚Äçüé§"}
names = {"human": "Voc√™", "ai": "lucIAna"}

# Itera sobre cada mensagem no hist√≥rico de mensagens
for idx, msg in enumerate(msgs.messages):  
    # Cria uma mensagem no chat com o avatar correspondente ao tipo de usu√°rio (humano ou IA)
    with st.chat_message(avatars[msg.type]):  
        st.write(names[msg.type])  # Adiciona o nome abaixo do avatar

        # Itera sobre os passos armazenados para cada mensagem, se houver
        for step in st.session_state.steps.get(str(idx), []):  
            # Se o passo atual indica uma exce√ß√£o, pula para o pr√≥ximo passo
            if step[0].tool == "_Exception":  
                continue

            # Cria um expander para cada ferramenta usada na resposta, mostrando o input
            with st.expander(f"‚úÖ **{step[0].tool}**: {step[0].tool_input}"): 
                # Exibe o log de execu√ß√£o da ferramenta 
                st.write(step[0].log)  
                # Exibe o resultado da execu√ß√£o da ferramenta
                st.write(f"**{step[1]}**")  

        # Exibe o conte√∫do da mensagem no chat
        st.write(msg.content)  

# Fun√ß√£o para verificar se a pergunta √© jur√≠dica
def is_legal_question(question):
    legal_keywords = ["lei", "contrato", "jur√≠dico", "advogado", "justi√ßa", "processo", "direito", "tribunal", "artigo", "bom dia", "boa tarde", "boa noite", "oi", "ol√°"]
    return any(keyword in question.lower() for keyword in legal_keywords)

# Fun√ß√£o para o chat da IA
def ia_chat():
    # Campo de entrada para novas mensagens do usu√°rio
    if prompt := st.chat_input(placeholder="Digite uma pergunta para come√ßar!", key="chat_input"):
        st.chat_message("user").write(prompt)
        
        if is_legal_question(prompt):
            # Configura√ß√£o do modelo de linguagem da Cohere
            llm = ChatCohere(cohere_api_key=cohere_api_key)
            
            # Configura√ß√£o da ferramenta de busca do agente
            mecanismo_busca = [DuckDuckGoSearchRun(name="Search")]
            
            # Cria√ß√£o do agente conversacional com a ferramenta de busca
            chat_dsa_agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=mecanismo_busca)
            
            # Executor para o agente, incluindo mem√≥ria e tratamento de erros
            executor = AgentExecutor.from_agent_and_tools(agent=chat_dsa_agent,
                                                          tools=mecanismo_busca,
                                                          memory=memory,
                                                          return_intermediate_steps=True,
                                                          handle_parsing_errors=True)

            # Defini√ß√£o do prompt template
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "Voc√™ √© um advogado experiente e deve fornecer respostas com base em seu conhecimento jur√≠dico."),
                ("user", "{input}")
            ])
            
            # Cria√ß√£o do chain
            chain = prompt_template | llm | StrOutputParser()

            # Execu√ß√£o do chain com a entrada do usu√°rio
            response = chain.invoke({"input": prompt})

            # Adicionar a resposta da IA ao hist√≥rico de mensagens
            msgs.add_ai_message(response)

            # Salvar a pergunta e resposta no arquivo JSON
            chat_history.append({"role": "user", "content": prompt})
            chat_history.append({"role": "ai", "content": response})
            save_data(json_file_path, chat_history)

            # Exibir a resposta do assistente
            with st.chat_message("ü§ñ"):
                st.write("lucIAna")  # Adiciona o nome abaixo do avatar
                st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)  
                response = executor(prompt, callbacks=[st_cb])
                st.write(response["output"])
                # Armazenamento dos passos intermedi√°rios
                st.session_state.steps[str(len(msgs.messages) - 1)] = response["intermediate_steps"]  
        else:
            # Resposta para perguntas n√£o jur√≠dicas
            response = "Desculpe, fui treinada apenas para responder perguntas sobre temas jur√≠dicos."
            msgs.add_ai_message(response)
            chat_history.append({"role": "user", "content": prompt})
            chat_history.append({"role": "ai", "content": response})
            save_data(json_file_path, chat_history)
            
            with st.chat_message("ü§ñ"):
                st.write("lucIAna")  # Adiciona o nome abaixo do avatar
                st.write(response)

# Fun√ß√£o para IA - Docs
def ia_docs():
    st.write("Fun√ß√£o para resumir documentos ainda em desenvolvimento.")
    
    # Fun√ß√£o para carregar e processar o documento PDF
    def load_doc(list_file_path, chunk_size, chunk_overlap):
        loaders = [PyPDFLoader(x) for x in list_file_path]
        pages = []
        for loader in loaders:
            pages.extend(loader.load())
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        doc_splits = text_splitter.split_documents(pages)
        return doc_splits

    # Fun√ß√£o para criar o banco de dados vetorial
    def create_db(splits):
        embeddings = HuggingFaceEmbeddings()
        vectordb = FAISS.from_documents(splits, embeddings)
        return vectordb

    # Fun√ß√£o para inicializar a base de dados vetorial
    def initialize_database(list_file_obj, chunk_size, chunk_overlap, progress=st.progress):
        list_file_path = [x.name for x in list_file_obj if x is not None]
        collection_name = create_collection_name(list_file_path[0])
        doc_splits = load_doc(list_file_path, chunk_size, chunk_overlap)
        vector_db = create_db(doc_splits)
        return vector_db, collection_name, "Complete!"

    # Fun√ß√£o para criar o nome da cole√ß√£o
    def create_collection_name(filepath):
        collection_name = Path(filepath).stem
        collection_name = collection_name.replace(" ", "-")
        collection_name = unidecode(collection_name)
        collection_name = re.sub('[^A-Za-z0-9]+', '-', collection_name)
        collection_name = collection_name[:50]
        if len(collection_name) < 3:
            collection_name = collection_name + 'xyz'
        if not collection_name[0].isalnum():
            collection_name = 'A' + collection_name[1:]
        if not collection_name[-1].isalnum():
            collection_name = collection_name[:-1] + 'Z'
        print('Filepath: ', filepath)
        print('Collection name: ', collection_name)
        return collection_name

    # Fun√ß√£o para inicializar o LLM chain usando Mistral v0.3 com a API da Hugging Face
    def initialize_llmchain(llm_model, temperature, max_tokens, top_k, vector_db, progress=st.progress):
        progress(0.1, "Initializing HF tokenizer...")
        progress(0.5, "Initializing HF Hub...")

        llm = HuggingFaceEndpoint(
            repo_id=llm_model,
            huggingfacehub_api_token=hf_api_key,
            temperature=temperature,
            max_new_tokens=max_tokens,
            top_k=top_k,
        )

        progress(0.75, "Defining buffer memory...")
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            output_key='answer',
            return_messages=True
        )
        retriever = vector_db.as_retriever()
        progress(0.8, "Defining retrieval chain...")
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm,
            retriever=retriever,
            chain_type="stuff", 
            memory=memory,
            return_source_documents=True,
            verbose=False,
        )
        progress(0.9, "Done!")
        return qa_chain

    # Inicializa√ß√£o dos elementos de interface para upload e processamento de documentos PDF
    uploaded_files = st.file_uploader("Upload your PDF documents (single or multiple)", type="pdf", accept_multiple_files=True)
    chunk_size = st.slider("Chunk size", 100, 1000, 600, 20)
    chunk_overlap = st.slider("Chunk overlap", 10, 200, 40, 10)

    if st.button("Generate vector database"):
        if uploaded_files:
            vector_db, collection_name, status = initialize_database(uploaded_files, chunk_size, chunk_overlap)
            st.success("Vector database created successfully!")

            # Inicializa√ß√£o do LLM chain
            llm_option = 1  # √çndice para Mistral v0.3
            llm_temperature = 0.7
            max_tokens = 1024
            top_k = 3

            qa_chain = initialize_llmchain("mistralai/Mistral-7B-Instruct-v0.3", llm_temperature, max_tokens, top_k, vector_db)
            st.success("LLM chain initialized successfully!")

# L√≥gica para escolher a fun√ß√£o baseada na op√ß√£o selecionada
if option == "IA - CHAT":
    ia_chat()
elif option == "IA - Docs":
    ia_docs()

