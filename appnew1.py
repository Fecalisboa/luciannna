# Pacote para manipula√ß√£o dos dados em formato JSON
import json

# Framework para cria√ß√£o de aplica√ß√µes web
import streamlit as st  

# Para cria√ß√£o e execu√ß√£o de agentes conversacionais
from langchain.agents import ConversationalChatAgent, AgentExecutor  

# Callback para intera√ß√£o com a interface do Streamlit
from langchain_community.callbacks import StreamlitCallbackHandler  

# Mem√≥ria para armazenar o hist√≥rico de conversa
from langchain.memory import ConversationBufferMemory  

# Hist√≥rico de mensagens para o Streamlit
from langchain_community.chat_message_histories import StreamlitChatMessageHistory 

# Integra√ß√£o com o modelo de linguagem da Cohere
from langchain_cohere import ChatCohere
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Ferramenta de busca DuckDuckGo para o agente 
from langchain_community.tools import DuckDuckGoSearchRun  

# Remove warnings
import warnings
warnings.filterwarnings('ignore')

# Pacotes para processamento de documentos PDF e cria√ß√£o de banco de dados vetorial
import os
from pathlib import Path
import re
from unidecode import unidecode
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_community.llms import HuggingFaceEndpoint

# Configura√ß√£o do t√≠tulo da p√°gina
st.set_page_config(page_title="lucIAna")

# Cria√ß√£o de colunas para layout da p√°gina
# Define a propor√ß√£o das colunas
col1, col4 = st.columns([4, 1])  

# Configura√ß√£o da primeira coluna para exibir o t√≠tulo do projeto
with col1:
    st.title("lucIAna")

# Defini√ß√£o da chave de API da Cohere
cohere_api_key = "OGY2ZCgZ4351TM0pXzRNeJLpw6o9GhyfWA3r05eW"

# Obtenha o token da vari√°vel de ambiente
hf_api_key = "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"

# Adi√ß√£o de bot√µes para diferentes funcionalidades
st.sidebar.header("Escolha uma op√ß√£o:")
option = st.sidebar.radio("Op√ß√µes", ["IA - CHAT", "IA - Docs"])

# Fun√ß√£o para carregar dados do arquivo JSON
def load_data(file_path):
    try:
        with open(file_path, 'r') as file:
            return json.load(file)
    except FileNotFoundError:
        return []

# Fun√ß√£o para salvar dados no arquivo JSON
def save_data(file_path, data):
    with open(file_path, 'w') as file:
        json.dump(data, file, indent=4)

# Caminho do arquivo JSON
json_file_path = 'chat_history.json'

# Carregar hist√≥rico de mensagens do arquivo JSON
chat_history = load_data(json_file_path)

# Inicializa√ß√£o do hist√≥rico de mensagens no Streamlit
if "msgs" not in st.session_state:
    st.session_state.msgs = StreamlitChatMessageHistory()

msgs = st.session_state.msgs

# Configura√ß√£o da mem√≥ria do chat
memory = ConversationBufferMemory(chat_memory=msgs, 
                                  return_messages=True, 
                                  memory_key="chat_history", 
                                  output_key="output")

# Verifica√ß√£o para limpar o hist√≥rico de mensagens ou iniciar a conversa
if len(msgs.messages) == 0 or st.sidebar.button("Reset", key="reset_button"):
    msgs.clear()
    msgs.add_ai_message("Sou sua Assistente Jur√≠dica, em que posso ajudar?")
    st.session_state.steps = {}

# Defini√ß√£o de avatares para os participantes da conversa
avatars = {"human": "user", "ai": "üë©‚Äçüé§"}
names = {"human": "Voc√™", "ai": "lucIAna"}

# Itera sobre cada mensagem no hist√≥rico de mensagens
for idx, msg in enumerate(msgs.messages):  
    # Cria uma mensagem no chat com o avatar correspondente ao tipo de usu√°rio (humano ou IA)
    with st.chat_message(avatars[msg.type]):  
        st.write(names[msg.type])  # Adiciona o nome abaixo do avatar

        # Itera sobre os passos armazenados para cada mensagem, se houver
        for step in st.session_state.steps.get(str(idx), []):  
            # Se o passo atual indica uma exce√ß√£o, pula para o pr√≥ximo passo
            if step[0].tool == "_Exception":  
                continue

            # Cria um expander para cada ferramenta usada na resposta, mostrando o input
            with st.expander(f"‚úÖ **{step[0].tool}**: {step[0].tool_input}"): 
                # Exibe o log de execu√ß√£o da ferramenta 
                st.write(step[0].log)  
                # Exibe o resultado da execu√ß√£o da ferramenta
                st.write(f"**{step[1]}**")  

        # Exibe o conte√∫do da mensagem no chat
        st.write(msg.content)  

# Fun√ß√£o para verificar se a pergunta √© jur√≠dica
def is_legal_question(question):
    legal_keywords = ["lei", "contrato", "jur√≠dico", "advogado", "justi√ßa", "processo", "direito", "tribunal", "artigo", "bom dia", "boa tarde", "boa noite", "oi", "ol√°"]
    return any(keyword in question.lower() for keyword in legal_keywords)

# Fun√ß√£o para o chat da IA
def ia_chat():
    # Campo de entrada para novas mensagens do usu√°rio
    if prompt := st.chat_input(placeholder="Digite uma pergunta para come√ßar!", key="chat_input"):
        st.chat_message("user").write(prompt)
        
        if is_legal_question(prompt):
            # Configura√ß√£o do modelo de linguagem da Cohere
            llm = ChatCohere(cohere_api_key=cohere_api_key)
            
            # Configura√ß√£o da ferramenta de busca do agente
            mecanismo_busca = [DuckDuckGoSearchRun(name="Search")]
            
            # Cria√ß√£o do agente conversacional com a ferramenta de busca
            chat_dsa_agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=mecanismo_busca)
            
            # Executor para o agente, incluindo mem√≥ria e tratamento de erros
            executor = AgentExecutor.from_agent_and_tools(agent=chat_dsa_agent,
                                                          tools=mecanismo_busca,
                                                          memory=memory,
                                                          return_intermediate_steps=True,
                                                          handle_parsing_errors=True)

            # Defini√ß√£o do prompt template
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "Voc√™ √© um advogado experiente e deve fornecer respostas com base em seu conhecimento jur√≠dico."),
                ("user", "{input}")
            ])
            
            # Cria√ß√£o do chain
            chain = prompt_template | llm | StrOutputParser()

            # Execu√ß√£o do chain com a entrada do usu√°rio
            response = chain.invoke({"input": prompt})

            # Adicionar a resposta da IA ao hist√≥rico de mensagens
            msgs.add_ai_message(response)

            # Salvar a pergunta e resposta no arquivo JSON
            chat_history.append({"role": "user", "content": prompt})
            chat_history.append({"role": "ai", "content": response})
            save_data(json_file_path, chat_history)

            # Exibir a resposta do assistente
            with st.chat_message("ü§ñ"):
                st.write("lucIAna")  # Adiciona o nome abaixo do avatar
                st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)  
                response = executor(prompt, callbacks=[st_cb])
                st.write(response["output"])
                # Armazenamento dos passos intermedi√°rios
                st.session_state.steps[str(len(msgs.messages) - 1)] = response["intermediate_steps"]  
        else:
            # Resposta para perguntas n√£o jur√≠dicas
            response = "Desculpe, fui treinada apenas para responder perguntas sobre temas jur√≠dicos."
            msgs.add_ai_message(response)
            chat_history.append({"role": "user", "content": prompt})
            chat_history.append({"role": "ai", "content": response})
            save_data(json_file_path, chat_history)
            
            with st.chat_message("ü§ñ"):
                st.write("lucIAna")  # Adiciona o nome abaixo do avatar
                st.write(response)

# Fun√ß√£o para IA - Docs
def ia_docs():
    st.write("Carregue e processe seus documentos PDF.")
    
    # Fun√ß√£o para carregar e processar o documento PDF
    list_llm = ["mistralai/Mistral-7B-Instruct-v0.2", "mistralai/Mistral-7B-Instruct-v0.3"]  
    list_llm_simple = [os.path.basename(llm) for llm in list_llm]
    
    # Load and split PDF document
    def load_doc(list_file_path):
        # Processing for one document only
        # loader = PyPDFLoader(file_path)
        # pages = loader.load()
        loaders = [PyPDFLoader(x) for x in list_file_path]
        pages = []
        for loader in loaders:
            pages.extend(loader.load())
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size = 1024, 
            chunk_overlap = 64 
        )  
        doc_splits = text_splitter.split_documents(pages)
        return doc_splits
    
    # Create vector database
    def create_db(splits):
        embeddings = HuggingFaceEmbeddings()
        vectordb = FAISS.from_documents(splits, embeddings)
        return vectordb
    
    
    # Initialize langchain LLM chain
    def initialize_llmchain(llm_model, temperature, max_tokens, top_k, vector_db, progress=gr.Progress()):
        if llm_model == "mistralai/Mistral-7B-Instruct-v0.2":
            llm = HuggingFaceEndpoint(
                repo_id=llm_model,
                huggingfacehub_api_token = api_token,
                temperature = temperature,
                max_new_tokens = max_tokens,
                top_k = top_k,
            )
        else:
            llm = HuggingFaceEndpoint(
                huggingfacehub_api_token = api_token,
                repo_id=llm_model, 
                temperature = temperature,
                max_new_tokens = max_tokens,
                top_k = top_k,
            )
        
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            output_key='answer',
            return_messages=True
        )
    
        retriever=vector_db.as_retriever()
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm,
            retriever=retriever,
            chain_type="stuff", 
            memory=memory,
            return_source_documents=True,
            verbose=False,
        )
        return qa_chain



# L√≥gica para escolher a fun√ß√£o baseada na op√ß√£o selecionada
if option == "IA - CHAT":
    ia_chat()
elif option == "IA - Docs":
    ia_docs()


